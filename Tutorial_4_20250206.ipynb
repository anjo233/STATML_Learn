{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Stellar Spectra\n",
    "\n",
    "- By Yuan-Sen Ting, January 2025\n",
    "\n",
    "In this tutorial, we'll apply linear regression to analyze stellar spectra from the Sloan Digital Sky Survey's Apache Point Observatory Galactic Evolution Experiment (SDSS APOGEE). Our goal is to predict stellar temperatures from spectral data, illustrating how linear regression can extract physical information from astronomical observations.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "### What You Should Already Know\n",
    "\n",
    "Before starting this lab, you should be familiar with:\n",
    "\n",
    "1. **Linear Regression & Maximum Likelihood**: You should understand how linear regression works through maximum likelihood estimation (MLE), as covered in the lecture.\n",
    "2. **NumPy Matrix Operations**: You'll need to be comfortable with matrices in NumPy for data manipulation.\n",
    "3. **Basic Statistics**: Understanding of concepts like mean, variance, and probability distributions.\n",
    "4. **Bayesian Concepts**: Basic familiarity with Bayesian statistics and prior distributions.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "After completing this lab, you'll be able to:\n",
    "\n",
    "1. **Apply Linear Regression to Stellar Data**: Learn how to use linear regression for analyzing APOGEE spectra.\n",
    "2. **Implement Maximum Likelihood**: Use MLE to find optimal model parameters.\n",
    "3. **Apply Regularization**: Use regularization techniques to prevent overfitting.\n",
    "\n",
    "Let's begin by setting up our Python environment with the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "# Set higher DPI for better looking plots\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "# For reproducibility \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "  \n",
    "We'll be working with 6,500 stellar spectra from APOGEE (Apache Point Observatory Galactic Evolution Experiment). APOGEE has been mapping out stars across the Milky Way, providing us with detailed spectroscopic data about their properties.\n",
    "  \n",
    "### Key Variables\n",
    "  \n",
    "1. **Spectra (`spectrum_array`)**: Raw spectral measurements for our 6,500 stars. Each spectrum has 7214 data points.\n",
    "\n",
    "2. **Wavelength (`wavelength`)**: The wavelength values corresponding to our spectral data points, helping us identify specific absorption lines.\n",
    "  \n",
    "3. **Effective Temperature (`teff_array`)**: Surface temperatures of the stars - this is what we'll try to predict with our regression model.\n",
    "    \n",
    "4. **Surface Gravity (`logg_array`)**: Log of surface gravity (in cgs units) for each star. We won't use this in today's analysis.\n",
    "  \n",
    "5. **Metallicity (`feh_array`)**: Measures how metal-rich each star is compared to the Sun. Also not used in this tutorial.\n",
    "   \n",
    "### Data Quality\n",
    "   \n",
    "For this tutorial, we're using high-quality spectra with mostly signal-to-noise ratios (SNR) > 50. While there are some minor measurement uncertainties of the spectra in our dataset, we'll assume the input is noiseless. This is because in our lecture, we only covered the case where there is noise in the output (target) variable, but not in the input features. In real astronomical data analysis, properly accounting for measurement uncertainties in both inputs and outputs often requires more sophisticated techniques.  Which we will cover in the Lecture 6. For now, we'll use the simpler approach to demonstrate the core concepts.\n",
    "  \n",
    "Let's load our data and take a look at what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from the file\n",
    "file = np.load('dataset_apogee_spectra.npz')\n",
    "\n",
    "# Extract spectral data and uncertainties\n",
    "spectra = file['spectrum_array']\n",
    "wavelength = file['wavelength']\n",
    "\n",
    "# Extract stellar parameters\n",
    "teff_array = file['teff_array']\n",
    "logg_array = file['logg_array']\n",
    "feh_array = file['feh_array']\n",
    "\n",
    "# Print basic information about our dataset\n",
    "print(f\"Number of stars: {len(spectra)}\")\n",
    "print(f\"Points per spectrum: {len(wavelength)}\")\n",
    "print(f\"Temperature range: {np.min(teff_array):.0f}K - {np.max(teff_array):.0f}K\")\n",
    "print(f\"\\nTypical uncertainty range in spectra:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing a Sample Spectrum\n",
    " \n",
    "Let's examine what our data actually looks like by plotting a spectrum. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a portion of the spectrum for the first star with uncertainties\n",
    "# We'll take only the first 1000 points for clearer visualization\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Plot the spectrum\n",
    "plt.errorbar(wavelength[:1000], spectra[0, :1000],\n",
    "             alpha=0.5, ecolor='gray', capsize=0)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(r'Wavelength ($\\mathrm{\\AA}$)')\n",
    "plt.ylabel('Normalized Flux')\n",
    "plt.title('Sample APOGEE Spectrum with Uncertainties')\n",
    "\n",
    "# Add a text box with stellar parameters\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "text = fr'$T_{{\\mathrm{{eff}}}}$ = {teff_array[0]:.0f} K' + '\\n' + \\\n",
    "       fr'$\\log g$ = {logg_array[0]:.2f}' + '\\n' + \\\n",
    "       fr'$[\\mathrm{{Fe/H}}]$ = {feh_array[0]:.2f}'\n",
    "plt.text(0.05, 0.2, text, transform=plt.gca().transAxes, \n",
    "         verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Our Sample: The Kiel Diagram\n",
    "\n",
    "Let's look at the Kiel diagram - it's a useful way to visualize stellar properties by plotting surface gravity against effective temperature. We'll also color-code the points by metallicity to add another dimension of information.\n",
    " \n",
    "Our APOGEE data doesn't have many dwarf stars. There are a few reasons for this:\n",
    " \n",
    "1. **APOGEE's Design**: The survey was built to study giant stars - its pipeline works best with them.\n",
    "   \n",
    "2. **Data Quality**: We're only using high signal-to-noise data here. Since dwarf stars are typically fainter, their spectra often don't make this quality cut.\n",
    "\n",
    "This visualization will help us understand the parameter space covered by our training data, which is important for understanding where our regression model will be most reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of Surface Gravity vs Effective Temperature, color-coded by Metallicity\n",
    "plt.figure(figsize=(8, 5))\n",
    "scatter = plt.scatter(teff_array, logg_array, c=feh_array, \n",
    "                     cmap='viridis', s=10, alpha=0.6)\n",
    "\n",
    "# Label axes and add title\n",
    "plt.xlabel('Effective Temperature [K]')\n",
    "plt.ylabel('Surface Gravity [log g]')\n",
    "plt.title('Kiel Diagram of APOGEE Sample')\n",
    "\n",
    "# Add a colorbar\n",
    "plt.colorbar(scatter, label='[Fe/H]')\n",
    "\n",
    "# Invert both axes for traditional presentation\n",
    "plt.gca().invert_xaxis()\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some basic statistics about our sample\n",
    "print(\"Sample Statistics:\")\n",
    "print(f\"Temperature range: {np.min(teff_array):.0f}K - {np.max(teff_array):.0f}K\")\n",
    "print(f\"Surface gravity range: {np.min(logg_array):.2f} - {np.max(logg_array):.2f}\")\n",
    "print(f\"Metallicity range: {np.min(feh_array):.2f} - {np.max(feh_array):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression: Starting Simple\n",
    "  \n",
    "Let's start with basic linear regression to predict effective temperature from stellar spectra. We'll use the spectra as our input features ($\\mathbf{x}$) and temperature as our target variable ($y$).\n",
    "  \n",
    "In linear regression, we often use the symbol $\\mathbf{\\Phi}$ (Phi) to represent our design matrix or feature matrix. While $\\mathbf{x}$ represents raw input features, $\\mathbf{\\Phi}$ typically denotes the features after any transformations or preprocessing. In our case, $\\mathbf{\\Phi}$ will contain our spectral data plus a bias term.\n",
    "  \n",
    "### Initial Simplifications\n",
    "  \n",
    "For our first attempt, we'll make a few simplifying assumptions:\n",
    " \n",
    "1. **Ignore Uncertainties**: We'll initially treat all measurements as equally reliable, ignoring the uncertainty information we have.\n",
    "2. **Limited Features**: Instead of using all 7214 wavelength points, we'll start with just the first 2000 pixels to keep things computationally manageable.\n",
    "3. **Add Bias Term**: We'll add a bias term to our model by augmenting our feature matrix $\\mathbf{\\Phi}$ with a column of ones.\n",
    " \n",
    "These simplifications will help us establish a baseline model before adding more sophisticated treatments of regularization and feature engineering.\n",
    " \n",
    "Let's prepare our data for regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spectra as the design matrix and limit to first 2000 pixels\n",
    "Phi = spectra[:,:2000]\n",
    "\n",
    "# Add a column of ones to the design matrix to act as the bias term\n",
    "Phi = np.hstack((Phi, np.ones((Phi.shape[0], 1))))\n",
    "print(\"Shape of design matrix Phi:\", Phi.shape)\n",
    "\n",
    "# Use effective temperature as the target variable\n",
    "t = teff_array\n",
    "print(\"Shape of target array t:\", t.shape)\n",
    "\n",
    "# Split data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "Phi_train, Phi_test, t_train, t_test = train_test_split(Phi, t, test_size=0.2, random_state=42)\n",
    "print(\"\\nTraining set size:\", Phi_train.shape[0])\n",
    "print(\"Test set size:\", Phi_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation in Linear Regression\n",
    "\n",
    "As we saw in the lecture, for linear regression with homogeneous noise (constant variance), the maximum likelihood estimate has an analytical solution:\n",
    "\n",
    "$$\\mathbf{w}_{\\text{ML}} = (\\mathbf{\\Phi}_{\\text{train}}^T \\mathbf{\\Phi}_{\\text{train}})^{-1} \\mathbf{\\Phi}_{\\text{train}}^T \\mathbf{t}_{\\text{train}}$$\n",
    "\n",
    "This solution emerges from maximizing the likelihood function under the assumption of Gaussian noise with constant variance. It provides a good starting point for our analysis.\n",
    "\n",
    "Let's implement this solution and examine how well it predicts stellar temperatures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Maximum Likelihood weights using the analytical solution\n",
    "w_ml = np.linalg.inv(Phi_train.T @ Phi_train) @ Phi_train.T @ t_train\n",
    "\n",
    "# Make predictions on both training and test sets\n",
    "t_pred_train = Phi_train @ w_ml\n",
    "t_pred_test = Phi_test @ w_ml\n",
    "\n",
    "# Calculate RMSE for both sets\n",
    "train_RMSE = np.sqrt(np.mean((t_train - t_pred_train)**2))\n",
    "test_RMSE = np.sqrt(np.mean((t_test - t_pred_test)**2))\n",
    "\n",
    "print(\"Training RMSE: {:.2f}K\".format(train_RMSE))\n",
    "print(\"Test RMSE: {:.2f}K\".format(test_RMSE))\n",
    "\n",
    "# Create scatter plot of predicted vs actual temperatures\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Training set\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(t_train, t_pred_train, alpha=0.5, s=1)\n",
    "plt.plot([t_train.min(), t_train.max()], [t_train.min(), t_train.max()], \n",
    "         'r--', label='Perfect Prediction')\n",
    "plt.xlabel('True Temperature [K]')\n",
    "plt.ylabel('Predicted Temperature [K]')\n",
    "plt.title('Training Set')\n",
    "plt.legend()\n",
    "\n",
    "# Test set\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(t_test, t_pred_test, alpha=0.5, s=1)\n",
    "plt.plot([t_test.min(), t_test.max()], [t_test.min(), t_test.max()], \n",
    "         'r--', label='Perfect Prediction')\n",
    "plt.xlabel('True Temperature [K]')\n",
    "plt.ylabel('Predicted Temperature [K]')\n",
    "plt.title('Test Set')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Our First Results\n",
    "\n",
    "Our simple linear regression model is performing reasonably well, achieving a precision of around 130K in temperature predictions. To put this in context:\n",
    "\n",
    "1. **Data Usage**: We're currently only using the first 2000 pixels of each spectrum (less than 1/3 of the available data) and treating all measurements as equally reliable.\n",
    "\n",
    "2. **Benchmark Performance**: Major spectroscopic surveys like APOGEE typically achieve temperature uncertainties of around 50-100K. While our simple model isn't quite at that level, it's encouraging that we're in the right ballpark considering how basic our approach is.\n",
    "\n",
    "3. **Room for Improvement**: Several factors could potentially improve our predictions:\n",
    "   - Including more spectral features\n",
    "   - Using regularization to prevent overfitting\n",
    "   - Engineering better features based on our knowledge of stellar physics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Effect of Regularization\n",
    "\n",
    "As discussed in the lecture, when building machine learning models, we often encounter overfitting - where our model gets too good at predicting the training data but fails to generalize well to new data. Regularization helps prevent this by adding a penalty term that discourages complex solutions.\n",
    "\n",
    "From the lecture, we saw that adding a zero-mean Gaussian prior with variance $\\eta^2$ leads to the regularized solution:\n",
    "\n",
    "$$\\mathbf{w}_{\\text{ML}} = (\\lambda \\mathbf{I} + \\mathbf{\\Phi}^T\\mathbf{\\Phi})^{-1}\\mathbf{\\Phi}^T\\mathbf{t}$$\n",
    "\n",
    "where $\\lambda = \\sigma^2/\\eta^2$ is our regularization parameter. This parameter controls the trade-off between:\n",
    "- Fitting the data well (small $\\lambda$)\n",
    "- Keeping the weights small to prevent overfitting (large $\\lambda$)\n",
    "\n",
    "Let's implement this regularized solution and explore how different values of $\\lambda$ affect our predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate regularized weights\n",
    "def w_ml_regularised(l):\n",
    "    return np.linalg.inv(l * np.eye(Phi_train.shape[1]) + Phi_train.T @ Phi_train) @ Phi_train.T @ t_train\n",
    "\n",
    "# Define a range of log-scaled lambda values\n",
    "log_lambdas = np.linspace(-2, 2, 16)\n",
    "\n",
    "# Initialize array to store results\n",
    "# First column: training RMSE, Second column: test RMSE\n",
    "results = np.zeros((len(log_lambdas), 2))\n",
    "\n",
    "# Loop over each hyperparameter value and train the model\n",
    "for ix, l in enumerate(log_lambdas):\n",
    "    # Compute the regularized weight vector\n",
    "    w_reg = w_ml_regularised(10**float(l))\n",
    "    \n",
    "    # Calculate RMSE for training and test sets\n",
    "    rmse_train = np.sqrt(np.mean((t_train - Phi_train @ w_reg)**2))\n",
    "    rmse_test = np.sqrt(np.mean((t_test - Phi_test @ w_reg)**2))\n",
    "    \n",
    "    # Store the results\n",
    "    results[ix, 0] = rmse_train\n",
    "    results[ix, 1] = rmse_test\n",
    "    \n",
    "# Plot the RMSE values as a function of lambda\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(log_lambdas, results[:,0], 'b-', label='Train')\n",
    "plt.plot(log_lambdas, results[:,1], 'r-', label='Test')\n",
    "plt.xlabel(r'log($\\lambda$)')\n",
    "plt.ylabel('RMSE [K]')\n",
    "plt.title('RMSE vs Regularization Parameter')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find best lambda and its performance\n",
    "best_lambda = 10**log_lambdas[np.argmin(results[:,1])]\n",
    "print(f\"Best lambda: {best_lambda:.2f}\")\n",
    "print(f\"Best test RMSE: {np.min(results[:,1]):.2f}K\")\n",
    "\n",
    "# Compute final predictions using best lambda\n",
    "w_best = w_ml_regularised(best_lambda)\n",
    "t_pred_test_reg = Phi_test @ w_best\n",
    "\n",
    "# Plot comparison of predictions\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(t_test, t_pred_test_reg, alpha=0.5, s=1)\n",
    "plt.plot([t_test.min(), t_test.max()], [t_test.min(), t_test.max()], \n",
    "         'r--', label='Perfect Prediction')\n",
    "plt.xlabel('True Temperature [K]')\n",
    "plt.ylabel('Predicted Temperature [K]')\n",
    "plt.title('Predictions with Optimal Regularization')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print comparison of all methods\n",
    "print(\"\\nRMSE Comparison:\")\n",
    "print(f\"Without Regularization: {test_RMSE:.2f}K\")\n",
    "print(f\"Regularized: {np.min(results[:,1]):.2f}K\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of Regularization\n",
    "\n",
    "The regularization results show a meaningful improvement in our model's performance:\n",
    "\n",
    "1. **Significant Improvement**: With optimal regularization, we achieve an RMSE of ~115K on the test set, a substantial improvement from our previous RMSE of ~130K. This ~10% improvement in prediction accuracy demonstrates the value of regularization in our spectral analysis.\n",
    "\n",
    "2. **Training vs Test Performance**: The plot of RMSE against λ shows the classic bias-variance trade-off:\n",
    "   - At very low λ values, we see overfitting (low training error but high test error)\n",
    "   - At very high λ values, we see underfitting (both training and test error increase)\n",
    "   - The optimal λ provides the best balance between these extremes\n",
    "\n",
    "3. **Model Comparison**:\n",
    "   - Basic Linear Regression: ~130K RMSE\n",
    "   - With Optimal Regularization: ~115K RMSE\n",
    "   - For context, APOGEE achieves uncertainties of around 50-100K in their pipeline\n",
    "\n",
    "This improvement suggests that our initial models were indeed overfitting the data, and that regularization helps by constraining the model weights to focus on the most relevant spectral features for temperature determination.\n",
    "\n",
    "In our next section, we'll explore whether we can further improve performance through feature engineering - transforming our input features based on our knowledge of stellar physics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending Linear Models with Basis Functions\n",
    "\n",
    "As discussed in the lecture, linear regression becomes more powerful when we transform our input features using basis functions. These transformations allow us to capture nonlinear relationships while maintaining the computational advantages of linear regression.\n",
    "\n",
    "For our spectral analysis, we'll explore quadratic features. \n",
    "\n",
    "Let's implement a quadratic basis function that takes each feature $\\mathbf{x}$ and creates two new features: $[\\mathbf{x}, \\mathbf{x}^2]$. This allows our model to capture both linear and quadratic relationships between spectral features and temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create quadratic features\n",
    "def phi_quadratic(x):    \n",
    "    return np.hstack((x, x**2))\n",
    "\n",
    "# Extend the feature set\n",
    "Phi_quad_train = np.array([phi_quadratic(Phi_train[i]) for i in range(Phi_train.shape[0])])\n",
    "Phi_quad_test = np.array([phi_quadratic(Phi_test[i]) for i in range(Phi_test.shape[0])])\n",
    "\n",
    "# Remove the extra bias term that got duplicated\n",
    "Phi_quad_train = Phi_quad_train[:,:-1]\n",
    "Phi_quad_test = Phi_quad_test[:,:-1]\n",
    "\n",
    "# First try without regularization\n",
    "w_unreg = np.linalg.inv(Phi_quad_train.T @ Phi_quad_train) @ Phi_quad_train.T @ t_train\n",
    "t_train_pred = Phi_quad_train @ w_unreg\n",
    "t_test_pred = Phi_quad_test @ w_unreg\n",
    "\n",
    "# Calculate RMSE\n",
    "train_rmse = np.sqrt(np.mean((t_train - t_train_pred)**2))\n",
    "test_rmse = np.sqrt(np.mean((t_test - t_test_pred)**2))\n",
    "\n",
    "print(\"RMSE with quadratic features (no regularization):\")\n",
    "print(f\"Train: {train_rmse:.2f}K\")\n",
    "print(f\"Test: {test_rmse:.2f}K\")\n",
    "\n",
    "# Now try with regularization\n",
    "def w_ml_regularised_quad(l):\n",
    "    return np.linalg.inv(l * np.eye(Phi_quad_train.shape[1]) + \n",
    "                        Phi_quad_train.T @ Phi_quad_train) @ Phi_quad_train.T @ t_train\n",
    "\n",
    "# Search over regularization parameters\n",
    "log_lambdas = np.linspace(-2, 2, 16)\n",
    "results_quad = np.zeros((len(log_lambdas), 2))\n",
    "\n",
    "for ix, l in enumerate(log_lambdas):\n",
    "    w_reg = w_ml_regularised_quad(10**float(l))\n",
    "    rmse_train = np.sqrt(np.mean((t_train - Phi_quad_train @ w_reg)**2))\n",
    "    rmse_test = np.sqrt(np.mean((t_test - Phi_quad_test @ w_reg)**2))\n",
    "    results_quad[ix, 0] = rmse_train\n",
    "    results_quad[ix, 1] = rmse_test\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(log_lambdas, results_quad[:,0], 'b-', label='Train')\n",
    "plt.plot(log_lambdas, results_quad[:,1], 'r-', label='Test')\n",
    "plt.xlabel(r'log($\\lambda$)')\n",
    "plt.ylabel('RMSE [K]')\n",
    "plt.title('RMSE vs Regularization Parameter (Quadratic Features)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find best lambda and its performance\n",
    "best_lambda_quad = 10**log_lambdas[np.argmin(results_quad[:,1])]\n",
    "print(f\"\\nBest lambda: {best_lambda_quad:.2f}\")\n",
    "print(f\"Best test RMSE with quadratic features: {np.min(results_quad[:,1]):.2f}K\")\n",
    "\n",
    "# Print comparison of all methods\n",
    "print(\"\\nFinal RMSE Comparison:\")\n",
    "print(f\"Linear (no regularization): {test_RMSE:.2f}K\")\n",
    "print(f\"Linear (with regularization): {np.min(results[:,1]):.2f}K\")\n",
    "print(f\"Quadratic (with regularization): {np.min(results_quad[:,1]):.2f}K\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Feature Engineering Results\n",
    "\n",
    "Our experiments with quadratic features reveal some important lessons about model complexity and overfitting:\n",
    "\n",
    "1. **Severe Overfitting Without Regularization**: \n",
    "   - Training RMSE: 17K\n",
    "   - Test RMSE: 225K\n",
    "   This dramatic difference between training and test performance is a textbook example of overfitting. Our model has learned to fit the training data almost perfectly (17K precision!) but fails catastrophically on new data.\n",
    "\n",
    "2. **Recovery with Regularization**: Adding regularization helps control this overfitting:\n",
    "   - Linear (no regularization): ~130K\n",
    "   - Linear (with regularization): ~110K\n",
    "   - Quadratic (with regularization): ~110K\n",
    "\n",
    "   While regularization helps recover reasonable performance, the quadratic features only provide a very modest improvement over the regularized linear model.\n",
    "\n",
    "3. **Insights**:\n",
    "   - Adding complexity (quadratic terms) without proper constraints (regularization) can dramatically hurt generalization\n",
    "   - Even with regularization, doubling our number of features only yields marginal improvements\n",
    "   - The simpler regularized linear model might be preferable given the minimal gains from added complexity\n",
    "\n",
    "This exercise perfectly illustrates why we need both:\n",
    "- Feature engineering to potentially capture more complex relationships\n",
    "- Regularization to prevent our more complex models from overfitting\n",
    "\n",
    "It also demonstrates that more complex models aren't always better - the modest improvement from quadratic features might not justify the increased computational cost and model complexity in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    " \n",
    "In this tutorial, we've explored linear regression through the lens of stellar spectroscopy, progressing from simple models to more sophisticated approaches. Let's summarize our key findings:\n",
    "\n",
    "### Model Evolution and Performance\n",
    "1. **Basic Linear Regression**: ~130K RMSE\n",
    "   - Simple to implement\n",
    "   - Reasonable baseline performance\n",
    "   - No protection against overfitting\n",
    "\n",
    "2. **Regularized Linear Model**: ~115K RMSE\n",
    "   - Significant improvement over baseline\n",
    "   - Prevents overfitting through controlled parameter constraints\n",
    "   - Good balance of complexity and performance\n",
    "\n",
    "3. **Quadratic Features**: \n",
    "   - Without regularization: Severe overfitting (17K train / 225K test RMSE)\n",
    "   - With regularization: ~110K RMSE\n",
    "   - Minimal improvement over linear model despite doubled complexity\n",
    "\n",
    "### Key Lessons Learned\n",
    "\n",
    "1. **The Power of Simplicity**\n",
    "   - Simple models with proper regularization can outperform more complex unregularized models\n",
    "   - Adding complexity needs to be justified by meaningful performance improvements\n",
    "\n",
    "2. **Importance of Validation**\n",
    "   - Training performance alone can be misleading (as seen in quadratic features)\n",
    "   - Need to check generalization performance on test set\n",
    "   - Large gaps between training and test performance signal overfitting\n",
    "\n",
    "3. **The Role of Domain Knowledge**\n",
    "   - Understanding your data (like homogeneous uncertainties in APOGEE)\n",
    "   - Realistic expectations for model performance (comparing to APOGEE pipeline's ~50-100K)\n",
    "   - Making informed choices about model complexity\n",
    "\n",
    "### Future Directions\n",
    "\n",
    "Potential ways to further improve this model could include:\n",
    "- Using more of the available spectral features (we only used first 2000 points)\n",
    "- Incorporating physical knowledge about specific temperature-sensitive spectral regions\n",
    "- Exploring other basis functions based on stellar physics\n",
    "- Implementing a full Bayesian treatment to get uncertainty estimates in our predictions\n",
    "\n",
    "This tutorial demonstrates how principles from the lecture - like maximum likelihood estimation, regularization, and feature engineering - apply to real astronomical data analysis challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
